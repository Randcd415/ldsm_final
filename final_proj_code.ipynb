{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>5</td><td>application_1544155017119_0006</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-47-42.us-east-2.compute.internal:20888/proxy/application_1544155017119_0006/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-36-179.us-east-2.compute.internal:8042/node/containerlogs/container_1544155017119_0006_01_000002/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, sum, col, split, isnan, column, count, mean, to_date, year, desc, max, min, rank, regexp_replace, dense_rank, substring, lpad, ltrim, concat, lit, when\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.regression import DecisionTreeRegressor, RandomForestRegressor, LinearRegression, GeneralizedLinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import RFormula \n",
    "\n",
    "\n",
    "#5 nodes (m4.2xlarge), each with 16 cores and 32 GB\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"final_proj\") \\\n",
    "    .config(\"spark.driver.cores\", 1) \\\n",
    "    .config(\"spark.executor.memory\", \"10G\") \\\n",
    "    .config(\"spark.executor.cores\", \"5\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in Urban Institute's linked HMDA and ACS data\n",
    "#obs are at the tract level\n",
    "data = spark.read.csv(\"s3://randfinalproj/tract_data.csv.gz\", header = True)\n",
    "\n",
    "#format tract ID to be mergable with other data\n",
    "data = data.withColumn(\"tract\", regexp_replace(col(\"tract\"), \"[.]\", \"\"))\n",
    "data = data.withColumn(\"tract_id\", concat(\"county\", \"tract\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select relevant variables\n",
    "data = data.select(col(\"state\"), col(\"county\"), col(\"year\"), col(\"tract_id\"), \\\n",
    "                     #col(\"VALUEH_P50\"), \\\n",
    "                     col(\"FARM_1\"), col(\"FARM_2\"), \\\n",
    "                     col(\"BUILTYR2_1\"), col(\"BUILTYR2_2\"), col(\"BUILTYR2_3\"), \\\n",
    "                     col(\"BUILTYR2_4\"), col(\"BUILTYR2_5\"), col(\"BUILTYR2_6\"), \\\n",
    "                     col(\"BUILTYR2_7\"), \n",
    "                     #col(\"BUILTYR2_8\"), \n",
    "                     col(\"BUILTYR2_9\"), \\\n",
    "                     col(\"BUILTYR2_10\"), col(\"BUILTYR2_11\"), col(\"BUILTYR2_12\"), \\\n",
    "                     col(\"BUILTYR2_13\"), col(\"BUILTYR2_14\"), col(\"BUILTYR2_15\"), \\\n",
    "                     col(\"MORTGAGE_0\"), col(\"MORTGAGE_1\"), col(\"MORTGAGE_3\"), col(\"MORTGAGE_4\"), \\\n",
    "                     col(\"FOODSTMP_0\"), col(\"FOODSTMP_1\"), col(\"FOODSTMP_2\"), \\\n",
    "                     #col(\"PROPINSR_P50\"), \\\n",
    "                     col(\"OWNERSHP_1\"), col(\"OWNERSHP_2\"), \\\n",
    "                     col(\"RACE1_HH_B\"), col(\"RACE1_HH_H\"), col(\"RACE1_HH_O\"), col(\"RACE1_HH_W\"), \\\n",
    "                     col(\"EMPSTAT_HH_1\"), col(\"EMPSTAT_HH_2\"), col(\"EMPSTAT_HH_3\"), col(\"EMPSTAT_HH_0\"), \\\n",
    "                     col(\"EDUC_HH_0\"), col(\"EDUC_HH_1\"), col(\"EDUC_HH_2\"), \\\n",
    "                     col(\"EDUC_HH_3\"), col(\"EDUC_HH_4\"), col(\"EDUC_HH_5\"), col(\"EDUC_HH_6\"), \\\n",
    "                     col(\"EDUC_HH_7\"), col(\"EDUC_HH_8\"), col(\"EDUC_HH_10\"), \\\n",
    "                     col(\"INCOME_P50\"), \\\n",
    "                     col(\"AMOUNT_P50\"), \\\n",
    "                     col(\"PURPOSE_1\"), col(\"PURPOSE_2\"), col(\"PURPOSE_3\"), \n",
    "                     #col(\"PURPOSE_4\"), \\\n",
    "                     )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in origin-destination data \n",
    "drive = spark.read.parquet('s3://lsdm-emr-util/lsdm-data/travel-times/drive_times.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in novel dataset (created by me on local machine with R) of tract IDs affected by disasters and years affected\n",
    "disaster = spark.read.csv(\"s3://randfinalproj/disaster.csv\", header = True)\n",
    "disaster = disaster.select(\"tract_id\", \"Year_After_Disaster\", \"Disaster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter origin-destination data to tracts just directly affected by disasters\n",
    "direct_expression = (drive[\"from_tract\"] == disaster[\"tract_id\"])\n",
    "direct_type = \"inner\"\n",
    "direct_affect = drive.join(disaster, direct_expression, direct_type).where(col(\"Disaster\") == 1) \\\n",
    "                       .select(\"tract_id\", \"Year_After_Disaster\") \\\n",
    "                       .withColumn(\"miles\", lit(0))\n",
    "\n",
    "#drop duplicates\n",
    "direct_affect = direct_affect.groupBy(\"tract_id\", \"Year_After_Disaster\").agg(min(\"miles\").alias(\"miles\"))\n",
    "\n",
    "#issue: right_outer = 363 and inner = 326... why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another filter of o-d data, now to \"remote\" tracts: within 150 miles of directly affected tracts\n",
    "remote_expression = (drive[\"to_tract\"] == direct_affect[\"tract_id\"]) & (drive[\"from_tract\"] != direct_affect[\"tract_id\"])\n",
    "remote_type = \"inner\"\n",
    "remote_affect = drive.join(direct_affect.select(\"tract_id\", \"Year_After_Disaster\"), remote_expression, remote_type) \\\n",
    "                     .select(\"from_tract\", \"Year_After_Disaster\", \"miles\") \\\n",
    "                     .withColumnRenamed(\"from_tract\", \"tract_id\")\n",
    "\n",
    "#remove remote tracts counted multiple times b/c within 150 miles of 2+ direct tracts\n",
    "#group by tract and year, keeping min distance away\n",
    "remote_affect = remote_affect.groupBy(\"tract_id\", \"Year_After_Disaster\").agg(min(\"miles\").alias(\"miles\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bind the remote and directly affected datasets together (by row)\n",
    "affected_tracts = direct_affect.union(remote_affect).withColumnRenamed(\"tract_id\", \"temp_tract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join affected_tract data to main data, only includes affected tracks, but for all years data available\n",
    "main_join = (data[\"tract_id\"] == affected_tracts[\"temp_tract\"])\n",
    "main_Type = \"inner\"\n",
    "data_disaster = data.join(affected_tracts, main_join, main_Type).drop(\"temp_tract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new column indicating when treatment (miles) should be turned on\n",
    "#note disaster year is actually year after disaster\n",
    "data_disaster = data_disaster.withColumn(\"Disaster\", when(col(\"year\") == col(\"Year_After_Disaster\"), 1) \\\n",
    "                         .otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in annual home price data from Federal Finance Housing Agency\n",
    "#obs are at the tract level\n",
    "home_price = spark.read.csv(\"s3://randfinalproj/HPI_AT_BDL_tract.csv\", header = True)\n",
    "home_price = home_price.withColumnRenamed(\"year\", \"home_year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join home data to main acs_hmda_disaster data\n",
    "home_join = (data_disaster[\"tract_id\"] == home_price[\"tract\"]) & (data_disaster[\"year\"] == home_price[\"home_year\"])\n",
    "home_type = \"inner\"\n",
    "disaster_home = data_disaster.join(home_price, home_join, home_type).drop(\"home_year\", \"tract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint\n",
    "disaster_home.write.parquet(\"s3://randfinalproj/disaster_home.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_home = spark.read.parquet(\"s3://randfinalproj/disaster_home.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some features: unemployment rate, percentage renting, percentage college educated, percentage minority\n",
    "disaster_home = disaster_home.withColumn(\"unemp_rate\", col(\"EMPSTAT_HH_2\") / (col(\"EMPSTAT_HH_1\") + col(\"EMPSTAT_HH_2\"))) \\\n",
    "                             .withColumn(\"rent_rate\", col(\"OWNERSHP_2\") / (col(\"OWNERSHP_1\") + col(\"OWNERSHP_2\"))) \\\n",
    "                             .withColumn(\"college_rate\", col(\"EDUC_HH_10\") / (col(\"EDUC_HH_0\") + col(\"EDUC_HH_1\") + \\\n",
    "                                                                              col(\"EDUC_HH_2\") + col(\"EDUC_HH_3\") + \\\n",
    "                                                                              col(\"EDUC_HH_4\") + col(\"EDUC_HH_5\") + \\\n",
    "                                                                              col(\"EDUC_HH_6\") + col(\"EDUC_HH_7\") + \\\n",
    "                                                                              col(\"EDUC_HH_8\") + col(\"EDUC_HH_10\"))) \\\n",
    "                            .withColumn(\"minority_rate\", (col(\"RACE1_HH_B\") + col(\"RACE1_HH_H\") + col(\"RACE1_HH_O\")) / \\\n",
    "                                        (col(\"RACE1_HH_B\") + col(\"RACE1_HH_H\") + col(\"RACE1_HH_O\") + col(\"RACE1_HH_W\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+----------+----+--------+----------+---------+----------+-------------+\n",
      "|annual_change|miles|state_abbr|year|Disaster|unemp_rate|rent_rate|INCOME_P50|minority_rate|\n",
      "+-------------+-----+----------+----+--------+----------+---------+----------+-------------+\n",
      "|            0|    0|         0|   0|       0|         0|        0|         0|            0|\n",
      "+-------------+-----+----------+----+--------+----------+---------+----------+-------------+"
     ]
    }
   ],
   "source": [
    "#filter dataset, cast appropriate variables which are strings to floats\n",
    "#leaving years and states as strings to serve as fixed effects\n",
    "disaster_home_filt = disaster_home.select(col(\"annual_change\").cast(\"float\"), col(\"miles\").cast(\"float\"), \\\n",
    "                                          col(\"state_abbr\"), col(\"year\"), col(\"Disaster\"), \\\n",
    "                                          col(\"unemp_rate\"), col(\"rent_rate\"), \n",
    "                                          col(\"INCOME_P50\").cast(\"float\"), col(\"minority_rate\"))\n",
    "\n",
    "disaster_home_filt = disaster_home_filt.where(col(\"annual_change\").isNotNull()) \\\n",
    "                                        .where(col(\"unemp_rate\").isNotNull()) \\\n",
    "                                        .where(col(\"INCOME_P50\").isNotNull()) \\\n",
    "                                        .where(col(\"minority_rate\").isNotNull())\n",
    "    \n",
    "#college rate has so many NULLs... why!\n",
    "#annual change has a lot too (7621/185488)\n",
    "#unemp: 37/185488\n",
    "#income: 494/185488\n",
    "#minor: 2603/185488\n",
    "disaster_home_filt.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in disaster_home_filt.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into test vs train\n",
    "sep = disaster_home_filt.randomSplit([0.7, 0.3], 99)\n",
    "train = sep[0]\n",
    "test = sep[1]\n",
    "\n",
    "#create Diff-in-Diff specification\n",
    "#annual change in median home price regressed on variables\n",
    "#treatment effect is year after disaster in affected tract, also interacted with miles away from disaster\n",
    "supervised = RFormula(formula=\"annual_change ~ Disaster + miles:Disaster + unemp_rate + rent_rate + INCOME_P50 + minority_rate + state_abbr + year\")\n",
    "\n",
    "train_trans = supervised.fit(train).transform(train)\n",
    "test_trans = supervised.fit(test).transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with config:\n",
      "\t {Param(parent='GeneralizedLinearRegression_423ab242c127f857f3ae', name='regParam', doc='regularization parameter (>= 0).'): 0.0}\n",
      "Achieved avg metric of:\n",
      "\t 6.12011577548574\n",
      "\n",
      "Model with config:\n",
      "\t {Param(parent='GeneralizedLinearRegression_423ab242c127f857f3ae', name='regParam', doc='regularization parameter (>= 0).'): 0.5}\n",
      "Achieved avg metric of:\n",
      "\t 6.1297596195314705\n",
      "\n",
      "Model with config:\n",
      "\t {Param(parent='GeneralizedLinearRegression_423ab242c127f857f3ae', name='regParam', doc='regularization parameter (>= 0).'): 1.0}\n",
      "Achieved avg metric of:\n",
      "\t 6.150528658315142"
     ]
    }
   ],
   "source": [
    "#estimate is a generalized linear regression\n",
    "glr = GeneralizedLinearRegression(featuresCol = 'features', labelCol = \"label\")\n",
    "\n",
    "#I create a hyperparamter grid with different values for the regularization term\n",
    "params = ParamGridBuilder() \\\n",
    "            .addGrid(glr.regParam, [0.0, 0.5, 1.0]).build()\n",
    "\n",
    "#I use root mean squared error for my evaluation metric\n",
    "evaluator = RegressionEvaluator(predictionCol = \"prediction\",\n",
    "                                labelCol = \"label\",\n",
    "                                metricName = \"rmse\")\n",
    "\n",
    "cv = CrossValidator(estimator = glr,\n",
    "                    estimatorParamMaps = params,\n",
    "                    evaluator = evaluator,\n",
    "                    numFolds = 3)\n",
    "\n",
    "cvModel = cv.fit(train_trans)\n",
    "\n",
    "for i in range(len(params)):\n",
    "    print(\"Model with config:\\n\\t {}\".format(params[i]))\n",
    "    print(\"Achieved avg metric of:\\n\\t {}\".format(cvModel.avgMetrics[i]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'GeneralizedLinearRegressionTrainingSummary' object has no attribute 'coefficients'\n",
      "Traceback (most recent call last):\n",
      "AttributeError: 'GeneralizedLinearRegressionTrainingSummary' object has no attribute 'coefficients'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#can't figure out how to get coefficients or pvalues!\n",
    "\n",
    "summary = cvModel.bestModel.summary\n",
    "print(\"Coefficients: \" + str(summary.coefficients))\n",
    "print(\"Coefficient Standard Errors: \" + str(summary.coefficientStandardErrors))\n",
    "print(\"P Values: \" + str(summary.pValues))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
